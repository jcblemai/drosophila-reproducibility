{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import utils\n",
    "\n",
    "from_database = False\n",
    "\n",
    "if from_database:\n",
    "    # Load tables from database\n",
    "    dfs = utils.load_all_tables()\n",
    "else:\n",
    "    # Open the file in binary read mode and load the pickle data\n",
    "    with open('preprocessed_data/dfs.pickle', 'rb') as f:\n",
    "        dfs = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if from_database:\n",
    "    # Save tables to pickle\n",
    "    with open('preprocessed_data/dfs.pickle', 'wb') as f:\n",
    "        pickle.dump(dfs, f)\n",
    "\n",
    "    # load it back with:\n",
    "    with open('preprocessed_data/dfs.pickle', 'rb') as f:\n",
    "        dfs = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Create dataframe claims with all the referenced table from the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "def clean_df(df):\n",
    "    columns_to_remove = ['user_id', 'orcid_user_id', \n",
    "                    'created_at', 'updated_at', 'assertion_updated_at', \n",
    "                    'workspace_id', 'user_id', 'doi', 'organism_id', # 'pmid', \n",
    "                    'all_tags_json', 'obsolete', 'ext', 'badge_classes','pluralize_title',\n",
    "                    'can_attach_file', 'refresh_side_panel', 'icon_classes', 'btn_classes']\n",
    "    patterns_to_remove = ['validated', 'filename', 'obsolete_article']\n",
    "    for col in columns_to_remove:\n",
    "        if col in df.columns:\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "    for pattern in patterns_to_remove:\n",
    "        cols = [c for c in df.columns if pattern in c]\n",
    "        df.drop(cols, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# preprocess: my unit here is the claims that are in assertion\n",
    "# check wich columns have _id\n",
    "claims = dfs[\"assertions\"]\n",
    "print(len(claims))\n",
    "claims = claims[claims[\"obsolete\"] == False]\n",
    "print(len(claims))\n",
    "claims = clean_df(claims)\n",
    "\n",
    "id_cols = [col for col in claims.columns if \"_id\" in col]\n",
    "print(id_cols)\n",
    "# merge the article columsn\n",
    "articles = clean_df(dfs[\"articles\"])\n",
    "articles = articles.rename(columns={\"id\": \"article_id\"})\n",
    "# We keep article ID for later\n",
    "claims = claims.merge(articles, on=\"article_id\", how=\"left\", suffixes=('', '_article'))#.drop(\"article_id\", axis=1)\n",
    "\n",
    "\n",
    "id_cols = [col for col in claims.columns if \"_id\" in col]\n",
    "print(id_cols)\n",
    "\n",
    "journals = clean_df(dfs[\"journals\"])\n",
    "journals = journals.drop('tag', axis=1).rename(columns={\"id\": \"journal_id\", \"name\": \"journal_name\"})\n",
    "claims = claims.merge(journals, on=\"journal_id\", how=\"left\", suffixes=('', '_journal')).drop(\"journal_id\", axis=1)\n",
    "\n",
    "# same for assertion_type\n",
    "assertion_types = clean_df(dfs[\"assertion_types\"])\n",
    "assertion_types = assertion_types.rename(columns={\"id\": \"assertion_type_id\", \"name\": \"assertion_type\"})\n",
    "claims = claims.merge(assertion_types, on=\"assertion_type_id\", how=\"left\", suffixes=('', '_assertion_type')).drop(\"assertion_type_id\", axis=1)\n",
    "\n",
    "# same for assessment_type_id\n",
    "assessment_types = clean_df(dfs[\"assessment_types\"])\n",
    "assessment_types = assessment_types.rename(columns={\"id\": \"assessment_type_id\", \"name\": \"assessment_type\"})\n",
    "claims = claims.merge(assessment_types, on=\"assessment_type_id\", how=\"left\", suffixes=('', '_assessment_type')).drop(\"assessment_type_id\", axis=1)\n",
    "\n",
    "id_cols = [col for col in claims.columns if \"_id\" in col]\n",
    "print(id_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims = claims.drop(['published_at', 'badge_tag_classes','description', 'additional_context', 'references_txt'], axis=1) # most not consistently used accross dataset\n",
    "claims = claims.set_index('id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_columns = claims.select_dtypes(include='object').columns\n",
    "print(string_columns)\n",
    "\n",
    "for col in string_columns:\n",
    "    claims[col] = claims[col].apply(lambda x: x.replace('&amp;', '&') if isinstance(x, str) else x)\n",
    "\n",
    "claims.to_csv('preprocessed_data/claims.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_string(s, max_length=20):\n",
    "    \"\"\"Truncate string to max_length characters.\"\"\"\n",
    "    if isinstance(s, str) and len(s) > max_length:\n",
    "        return s[:max_length] + '...'\n",
    "    return s\n",
    "\n",
    "string_columns = string_columns.drop([\"assessment_type\"])\n",
    "\n",
    "df_truncated = claims.copy()\n",
    "\n",
    "for col in string_columns:\n",
    "    if col in df_truncated.columns:\n",
    "        df_truncated[col] = df_truncated[col].apply(lambda x: truncate_string(x))\n",
    "\n",
    "# Save truncated dataframe\n",
    "df_truncated.to_csv('preprocessed_data/claims_truncated_for_llm.csv', index=False)\n",
    "df_truncated\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
