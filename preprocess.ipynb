{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import preprocess_utils\n",
    "\n",
    "from_database = False\n",
    "\n",
    "if from_database:\n",
    "    # Load tables from database \n",
    "    dfs = preprocess_utils.load_all_tables()\n",
    "else:\n",
    "    # Open the file in binary read mode and load the pickle data\n",
    "    with open('preprocessed_data/dfs.pickle', 'rb') as f:\n",
    "        dfs = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if from_database:\n",
    "    # Save tables to pickle\n",
    "    with open('preprocessed_data/dfs.pickle', 'wb') as f:\n",
    "        pickle.dump(dfs, f)\n",
    "\n",
    "    # load it back with:\n",
    "    with open('preprocessed_data/dfs.pickle', 'rb') as f:\n",
    "        dfs = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Create dataframe claims with all the referenced table from the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13941\n",
      "13299\n",
      "['article_id', 'assertion_type_id', 'assessment_type_id']\n",
      "['article_id', 'assertion_type_id', 'assessment_type_id', 'journal_id']\n",
      "['article_id']\n"
     ]
    }
   ],
   "source": [
    "def clean_df(df):\n",
    "    # Create a copy of the DataFrame at the start\n",
    "    df = df.copy()\n",
    "    \n",
    "    columns_to_remove = ['user_id', 'orcid_user_id', \n",
    "                    'created_at', 'updated_at', 'assertion_updated_at', \n",
    "                    'workspace_id', 'user_id', 'doi', 'organism_id', # 'pmid', \n",
    "                    'all_tags_json', 'obsolete', 'ext', 'badge_classes','pluralize_title',\n",
    "                    'can_attach_file', 'refresh_side_panel', 'icon_classes', 'btn_classes']\n",
    "    patterns_to_remove = ['validated', 'filename', 'obsolete_article']\n",
    "    \n",
    "    # Remove existing columns\n",
    "    existing_cols = [col for col in columns_to_remove if col in df.columns]\n",
    "    if existing_cols:\n",
    "        df = df.drop(existing_cols, axis=1)\n",
    "    \n",
    "    # Remove pattern-matched columns\n",
    "    pattern_cols = []\n",
    "    for pattern in patterns_to_remove:\n",
    "        pattern_cols.extend([c for c in df.columns if pattern in c])\n",
    "    if pattern_cols:\n",
    "        df = df.drop(pattern_cols, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Main processing\n",
    "claims = dfs[\"assertions\"].copy()\n",
    "print(len(claims))\n",
    "claims = claims[claims[\"obsolete\"] == False].copy()\n",
    "print(len(claims))\n",
    "claims = clean_df(claims)\n",
    "\n",
    "id_cols = [col for col in claims.columns if \"_id\" in col]\n",
    "print(id_cols)\n",
    "\n",
    "# Process articles\n",
    "articles = clean_df(dfs[\"articles\"])\n",
    "articles = articles.rename(columns={\"id\": \"article_id\"})\n",
    "claims = claims.merge(articles, on=\"article_id\", how=\"left\", suffixes=('', '_article'))\n",
    "\n",
    "id_cols = [col for col in claims.columns if \"_id\" in col]\n",
    "print(id_cols)\n",
    "\n",
    "# Process journals\n",
    "journals = clean_df(dfs[\"journals\"])\n",
    "journals = journals.drop('tag', axis=1).rename(columns={\"id\": \"journal_id\", \"name\": \"journal_name\"})\n",
    "claims = claims.merge(journals, on=\"journal_id\", how=\"left\", suffixes=('', '_journal')).drop(\"journal_id\", axis=1)\n",
    "\n",
    "# Process assertion types\n",
    "assertion_types = clean_df(dfs[\"assertion_types\"])\n",
    "assertion_types = assertion_types.rename(columns={\"id\": \"assertion_type_id\", \"name\": \"assertion_type\"})\n",
    "claims = claims.merge(assertion_types, on=\"assertion_type_id\", how=\"left\", suffixes=('', '_assertion_type')).drop(\"assertion_type_id\", axis=1)\n",
    "\n",
    "# Process assessment types\n",
    "assessment_types = clean_df(dfs[\"assessment_types\"])\n",
    "assessment_types = assessment_types.rename(columns={\"id\": \"assessment_type_id\", \"name\": \"assessment_type\"})\n",
    "claims = claims.merge(assessment_types, on=\"assessment_type_id\", how=\"left\", suffixes=('', '_assessment_type')).drop(\"assessment_type_id\", axis=1)\n",
    "\n",
    "id_cols = [col for col in claims.columns if \"_id\" in col]\n",
    "print(id_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims = claims.drop(['published_at', 'badge_tag_classes','description', 'additional_context', 'references_txt'], axis=1) # most not consistently used accross dataset\n",
    "claims = claims.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update impact factor for \"Proceedings. Biological sciences\" and standardize the journal name\n",
    "claims.loc[claims[\"journal_name\"] == \"Proceedings. Biological sciences\", \"impact_factor\"] = 4.7 \n",
    "claims.loc[claims[\"journal_name\"] == \"Proceedings. Biological sciences\", \"journal_name\"] = \"Proceedings Biological Sciences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['content', 'pmid', 'authors_txt', 'title', 'volume', 'issue',\n",
      "       'abstract', 'key', 'affs_json', 'large_scale', 'journal_name',\n",
      "       'assertion_type', 'label', 'assessment_type'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "string_columns = claims.select_dtypes(include='object').columns\n",
    "print(string_columns)\n",
    "\n",
    "for col in string_columns:\n",
    "    claims[col] = claims[col].apply(lambda x: x.replace('&amp;', '&') if isinstance(x, str) else x)\n",
    "\n",
    "claims.to_csv('preprocessed_data/claims.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rank</th>\n",
       "      <th>pmid</th>\n",
       "      <th>authors_txt</th>\n",
       "      <th>title</th>\n",
       "      <th>pmid_article</th>\n",
       "      <th>volume</th>\n",
       "      <th>issue</th>\n",
       "      <th>abstract</th>\n",
       "      <th>...</th>\n",
       "      <th>large_scale</th>\n",
       "      <th>nber_tables</th>\n",
       "      <th>nber_panels</th>\n",
       "      <th>journal_name</th>\n",
       "      <th>impact_factor</th>\n",
       "      <th>assertion_type</th>\n",
       "      <th>label</th>\n",
       "      <th>is_assessed</th>\n",
       "      <th>assessment_type</th>\n",
       "      <th>rank_assessment_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5838</th>\n",
       "      <td>&lt;p&gt;Belozerov VE, Lin...</td>\n",
       "      <td>2049.0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>Zhuang ZH;Zhou Y;Yu ...</td>\n",
       "      <td>Regulation of Drosop...</td>\n",
       "      <td>16014325.0</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>The p38 mitogen-acti...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Cellular signalling</td>\n",
       "      <td>4.8</td>\n",
       "      <td>reference</td>\n",
       "      <td>Reference</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td></td>\n",
       "      <td>2517.0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>Choe KM;Werner T;StÃ¶...</td>\n",
       "      <td>Requirement for a pe...</td>\n",
       "      <td>11872802.0</td>\n",
       "      <td>296</td>\n",
       "      <td>5566</td>\n",
       "      <td>Components of microb...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Science (New York, N.Y.)</td>\n",
       "      <td>56.9</td>\n",
       "      <td>assessment</td>\n",
       "      <td>Assessment</td>\n",
       "      <td>False</td>\n",
       "      <td>Not assessed</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td></td>\n",
       "      <td>2517.0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>Choe KM;Werner T;StÃ¶...</td>\n",
       "      <td>Requirement for a pe...</td>\n",
       "      <td>11872802.0</td>\n",
       "      <td>296</td>\n",
       "      <td>5566</td>\n",
       "      <td>Components of microb...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Science (New York, N.Y.)</td>\n",
       "      <td>56.9</td>\n",
       "      <td>assessment</td>\n",
       "      <td>Assessment</td>\n",
       "      <td>False</td>\n",
       "      <td>Not assessed</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5840</th>\n",
       "      <td>Verified by many sub...</td>\n",
       "      <td>2204.0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>Zettervall CJ;Anderl...</td>\n",
       "      <td>A directed screen fo...</td>\n",
       "      <td>15381778.0</td>\n",
       "      <td>101</td>\n",
       "      <td>39</td>\n",
       "      <td>An attack by a paras...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Proceedings of the National Academy of Science...</td>\n",
       "      <td>11.1</td>\n",
       "      <td>assessment</td>\n",
       "      <td>Assessment</td>\n",
       "      <td>False</td>\n",
       "      <td>Verified</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5839</th>\n",
       "      <td>Activation of a cell...</td>\n",
       "      <td>2204.0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>Zettervall CJ;Anderl...</td>\n",
       "      <td>A directed screen fo...</td>\n",
       "      <td>15381778.0</td>\n",
       "      <td>101</td>\n",
       "      <td>39</td>\n",
       "      <td>An attack by a paras...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Proceedings of the National Academy of Science...</td>\n",
       "      <td>11.1</td>\n",
       "      <td>main_claim</td>\n",
       "      <td>Main claim</td>\n",
       "      <td>True</td>\n",
       "      <td>Verified</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14468</th>\n",
       "      <td>The DIF antiserum us...</td>\n",
       "      <td>2898.0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>Williams MJ;Rodrigue...</td>\n",
       "      <td>The 18-wheeler mutat...</td>\n",
       "      <td>9321392.0</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>Mammals and insects ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>The EMBO journal</td>\n",
       "      <td>11.4</td>\n",
       "      <td>comment</td>\n",
       "      <td>Comment</td>\n",
       "      <td>False</td>\n",
       "      <td>Not assessed</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11330</th>\n",
       "      <td>Although independent...</td>\n",
       "      <td>1577.0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>Shia AK;Glittenberg ...</td>\n",
       "      <td>Toll-dependent antim...</td>\n",
       "      <td>19934223.0</td>\n",
       "      <td>122</td>\n",
       "      <td>Pt 24</td>\n",
       "      <td>In Drosophila, the h...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>Journal of cell science</td>\n",
       "      <td>4.0</td>\n",
       "      <td>assessment</td>\n",
       "      <td>Assessment</td>\n",
       "      <td>False</td>\n",
       "      <td>Unchallenged, logically consistent</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11329</th>\n",
       "      <td>Spatzle (#gene:FBgn0...</td>\n",
       "      <td>1577.0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>Shia AK;Glittenberg ...</td>\n",
       "      <td>Toll-dependent antim...</td>\n",
       "      <td>19934223.0</td>\n",
       "      <td>122</td>\n",
       "      <td>Pt 24</td>\n",
       "      <td>In Drosophila, the h...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>Journal of cell science</td>\n",
       "      <td>4.0</td>\n",
       "      <td>main_claim</td>\n",
       "      <td>Main claim</td>\n",
       "      <td>True</td>\n",
       "      <td>Unchallenged, logically consistent</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11350</th>\n",
       "      <td>Although independent...</td>\n",
       "      <td>1577.0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>Shia AK;Glittenberg ...</td>\n",
       "      <td>Toll-dependent antim...</td>\n",
       "      <td>19934223.0</td>\n",
       "      <td>122</td>\n",
       "      <td>Pt 24</td>\n",
       "      <td>In Drosophila, the h...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>Journal of cell science</td>\n",
       "      <td>4.0</td>\n",
       "      <td>assessment</td>\n",
       "      <td>Assessment</td>\n",
       "      <td>False</td>\n",
       "      <td>Unchallenged, logically consistent</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11349</th>\n",
       "      <td>Spatzle (#gene:FBgn0...</td>\n",
       "      <td>1577.0</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>Shia AK;Glittenberg ...</td>\n",
       "      <td>Toll-dependent antim...</td>\n",
       "      <td>19934223.0</td>\n",
       "      <td>122</td>\n",
       "      <td>Pt 24</td>\n",
       "      <td>In Drosophila, the h...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>Journal of cell science</td>\n",
       "      <td>4.0</td>\n",
       "      <td>major_claim</td>\n",
       "      <td>Major claim</td>\n",
       "      <td>True</td>\n",
       "      <td>Unchallenged, logically consistent</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13299 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       content  article_id  rank  pmid  \\\n",
       "id                                                       \n",
       "5838   <p>Belozerov VE, Lin...      2049.0     1  None   \n",
       "693                                 2517.0     1  None   \n",
       "695                                 2517.0     1  None   \n",
       "5840   Verified by many sub...      2204.0     1  None   \n",
       "5839   Activation of a cell...      2204.0     1  None   \n",
       "...                        ...         ...   ...   ...   \n",
       "14468  The DIF antiserum us...      2898.0     1  None   \n",
       "11330  Although independent...      1577.0     1  None   \n",
       "11329  Spatzle (#gene:FBgn0...      1577.0     1  None   \n",
       "11350  Although independent...      1577.0     1  None   \n",
       "11349  Spatzle (#gene:FBgn0...      1577.0     3  None   \n",
       "\n",
       "                   authors_txt                    title  pmid_article volume  \\\n",
       "id                                                                             \n",
       "5838   Zhuang ZH;Zhou Y;Yu ...  Regulation of Drosop...    16014325.0     18   \n",
       "693    Choe KM;Werner T;StÃ¶...  Requirement for a pe...    11872802.0    296   \n",
       "695    Choe KM;Werner T;StÃ¶...  Requirement for a pe...    11872802.0    296   \n",
       "5840   Zettervall CJ;Anderl...  A directed screen fo...    15381778.0    101   \n",
       "5839   Zettervall CJ;Anderl...  A directed screen fo...    15381778.0    101   \n",
       "...                        ...                      ...           ...    ...   \n",
       "14468  Williams MJ;Rodrigue...  The 18-wheeler mutat...     9321392.0     16   \n",
       "11330  Shia AK;Glittenberg ...  Toll-dependent antim...    19934223.0    122   \n",
       "11329  Shia AK;Glittenberg ...  Toll-dependent antim...    19934223.0    122   \n",
       "11350  Shia AK;Glittenberg ...  Toll-dependent antim...    19934223.0    122   \n",
       "11349  Shia AK;Glittenberg ...  Toll-dependent antim...    19934223.0    122   \n",
       "\n",
       "       issue                 abstract  ...  large_scale nber_tables  \\\n",
       "id                                     ...                            \n",
       "5838       4  The p38 mitogen-acti...  ...        False         0.0   \n",
       "693     5566  Components of microb...  ...        False         NaN   \n",
       "695     5566  Components of microb...  ...        False         NaN   \n",
       "5840      39  An attack by a paras...  ...        False         2.0   \n",
       "5839      39  An attack by a paras...  ...        False         2.0   \n",
       "...      ...                      ...  ...          ...         ...   \n",
       "14468     20  Mammals and insects ...  ...        False         0.0   \n",
       "11330  Pt 24  In Drosophila, the h...  ...        False         1.0   \n",
       "11329  Pt 24  In Drosophila, the h...  ...        False         1.0   \n",
       "11350  Pt 24  In Drosophila, the h...  ...        False         1.0   \n",
       "11349  Pt 24  In Drosophila, the h...  ...        False         1.0   \n",
       "\n",
       "      nber_panels                                       journal_name  \\\n",
       "id                                                                     \n",
       "5838         14.0                                Cellular signalling   \n",
       "693           NaN                           Science (New York, N.Y.)   \n",
       "695           NaN                           Science (New York, N.Y.)   \n",
       "5840          9.0  Proceedings of the National Academy of Science...   \n",
       "5839          9.0  Proceedings of the National Academy of Science...   \n",
       "...           ...                                                ...   \n",
       "14468        27.0                                   The EMBO journal   \n",
       "11330        27.0                            Journal of cell science   \n",
       "11329        27.0                            Journal of cell science   \n",
       "11350        27.0                            Journal of cell science   \n",
       "11349        27.0                            Journal of cell science   \n",
       "\n",
       "      impact_factor  assertion_type        label is_assessed  \\\n",
       "id                                                             \n",
       "5838            4.8       reference    Reference       False   \n",
       "693            56.9      assessment   Assessment       False   \n",
       "695            56.9      assessment   Assessment       False   \n",
       "5840           11.1      assessment   Assessment       False   \n",
       "5839           11.1      main_claim   Main claim        True   \n",
       "...             ...             ...          ...         ...   \n",
       "14468          11.4         comment      Comment       False   \n",
       "11330           4.0      assessment   Assessment       False   \n",
       "11329           4.0      main_claim   Main claim        True   \n",
       "11350           4.0      assessment   Assessment       False   \n",
       "11349           4.0     major_claim  Major claim        True   \n",
       "\n",
       "                          assessment_type rank_assessment_type  \n",
       "id                                                              \n",
       "5838                                  NaN                  NaN  \n",
       "693                          Not assessed                 13.0  \n",
       "695                          Not assessed                 13.0  \n",
       "5840                             Verified                  1.0  \n",
       "5839                             Verified                  1.0  \n",
       "...                                   ...                  ...  \n",
       "14468                        Not assessed                 13.0  \n",
       "11330  Unchallenged, logically consistent                  6.0  \n",
       "11329  Unchallenged, logically consistent                  6.0  \n",
       "11350  Unchallenged, logically consistent                  6.0  \n",
       "11349  Unchallenged, logically consistent                  6.0  \n",
       "\n",
       "[13299 rows x 24 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def truncate_string(s, max_length=20):\n",
    "    \"\"\"Truncate string to max_length characters.\"\"\"\n",
    "    if isinstance(s, str) and len(s) > max_length:\n",
    "        return s[:max_length] + '...'\n",
    "    return s\n",
    "\n",
    "string_columns = string_columns.drop([\"assessment_type\", \"journal_name\"])\n",
    "\n",
    "df_truncated = claims.copy()\n",
    "\n",
    "for col in string_columns:\n",
    "    if col in df_truncated.columns:\n",
    "        df_truncated[col] = df_truncated[col].apply(lambda x: truncate_string(x))\n",
    "\n",
    "# Save truncated dataframe\n",
    "df_truncated.to_csv('preprocessed_data/claims_truncated_for_llm.csv', index=False)\n",
    "df_truncated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess the author files\n",
    "### A. Last authors\n",
    " It seems that\n",
    " - sex -> FH\n",
    " - PhD Post-doc -> FH\n",
    " - Become a Pi -> FH\n",
    " - current job -> FH\n",
    " - MD -> **???**\n",
    " - Affiliation -> Both\n",
    " - Country -> Both\n",
    " - Ivy League -> Both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read an xlx file in pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import  preprocess_utils\n",
    "\n",
    "claims_df = pd.read_csv('preprocessed_data/claims_truncated_for_llm.csv')\n",
    "major_claims_df = claims_df[claims_df['assertion_type'] == 'major_claim']\n",
    "\n",
    "# Read Author info, which contains all the pairs\n",
    "paper_auth_pairs = pd.read_excel('input_data/2025-02-14_last_xlsx/1_Triage_Last author.xlsx', sheet_name='Tri sans les doublons')\n",
    "# Drop all columns with 'Unnamed' in the name\n",
    "paper_auth_pairs = paper_auth_pairs.drop(columns=[col for col in paper_auth_pairs.columns if 'Unnamed' in col]).drop(columns=['Source'])\n",
    "paper_auth_pairs.to_csv('input_data/2025-02-14_last_xlsx/1_Triage_Last author.csv', index=False)\n",
    "\n",
    "first_authors_claims = pd.read_excel('input_data/2025-03-11/2025_March 9th_stats_author.xlsx', sheet_name='First')\n",
    "leading_authors_claims = pd.read_excel('input_data/2025-03-11/2025_March 9th_stats_author.xlsx', sheet_name='Leading')\n",
    "leading_authors_claims[\"Authorship\"]= \"Leading\"\n",
    "first_authors_claims[\"Authorship\"]= \"First\"\n",
    "\n",
    "authors_claims = pd.concat([leading_authors_claims, first_authors_claims])\n",
    "authors_claims['Sex'] = authors_claims['Sex'].map({1: 'Male', 0: 'Female'})\n",
    "authors_claims = authors_claims.drop(columns=[col for col in authors_claims.columns if '%' in col])\n",
    "authors_claims = authors_claims.drop(columns=[col for col in authors_claims.columns if 'Unnamed' in col])\n",
    "\n",
    "authors_claims = authors_claims.rename(columns={'Conituinity': 'Continuity', \"Partially verified\":\"Partially Verified\"})\n",
    "\n",
    "authors_claims['Historical lab'] = authors_claims['Historical lab'].astype('boolean')\n",
    "authors_claims['Continuity'] = authors_claims['Continuity'].astype('boolean')\n",
    "\n",
    "authors_claims.to_csv('input_data/2025-02-14_last_xlsx/stats_author.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_auth_pairs_LH = paper_auth_pairs[[\"last author\", \"Affiliation\", \"Country\", \"Ivy league\"]]\n",
    "paper_auth_pairs_LH = preprocess_utils.deduplicate_by(paper_auth_pairs_LH, \"last author\").copy()\n",
    "claims_LH = authors_claims[authors_claims['Authorship'] == 'Leading'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159 161 164\n"
     ]
    }
   ],
   "source": [
    "# Create merge columns: lowercased and stripped of accents\n",
    "# For paper_auth_pairs_LH\n",
    "paper_auth_pairs_LH.loc[:, 'lh_proc'] = paper_auth_pairs_LH['last author'].str.lower()\n",
    "paper_auth_pairs_LH.loc[:, 'lh_proc'] = (paper_auth_pairs_LH['lh_proc']\n",
    "    .str.normalize('NFKD')\n",
    "    .str.encode('ascii', errors='ignore')\n",
    "    .str.decode('utf-8'))\n",
    "\n",
    "# For claims_LH\n",
    "claims_LH.loc[:, 'lh_proc'] = claims_LH['Name'].str.lower()\n",
    "claims_LH.loc[:, 'lh_proc'] = claims_LH['lh_proc'].fillna('')\n",
    "claims_LH.loc[:, 'lh_proc'] = (claims_LH['lh_proc']\n",
    "    .str.normalize('NFKD')\n",
    "    .str.encode('ascii', errors='ignore')\n",
    "    .str.decode('utf-8'))\n",
    "claims_LH.loc[:, 'lh_proc'] = claims_LH['lh_proc'].str.replace('ando i', 'ando')\n",
    "\n",
    "# Perform the merge\n",
    "all_LH = pd.merge(claims_LH, paper_auth_pairs_LH, on='lh_proc', how='outer')\n",
    "print(len(claims_LH), len(paper_auth_pairs_LH), len(all_LH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¥ bellotti ra          Bellotti RA           nan\n",
      "ðŸ’¥ rizki mt             RIZKI MT              nan\n",
      "ðŸ’¥ shahabuddin m        Shahabuddin M         nan\n",
      "ðŸ’¥ shirasu-hiza mm      Shirasu-Hiza MM       nan\n",
      "ðŸ’¥ silvers mj           Silvers MJ            nan\n",
      "ðŸ’¥ schneider ds+d       nan                   Schneider DS+D\n",
      "ðŸ’¥ nappi aj             nan                   Nappi AJ\n",
      "ðŸ’¥                      nan                   nan\n"
     ]
    }
   ],
   "source": [
    "unique_pairs = all_LH[[\"Name\", \"last author\", \"lh_proc\"]].drop_duplicates().sort_values(\"last author\", ascending=True)\n",
    "for i in range(0, len(unique_pairs)):\n",
    "    if pd.isna(unique_pairs.iloc[i]['last author']) or pd.isna(unique_pairs.iloc[i]['Name']):\n",
    "        print('ðŸ’¥ ', end='')\n",
    "        print(f\"{unique_pairs.iloc[i]['lh_proc']:<20} {unique_pairs.iloc[i]['last author']:<20}  {unique_pairs.iloc[i]['Name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n"
     ]
    }
   ],
   "source": [
    "all_LH_inner = pd.merge(claims_LH, paper_auth_pairs_LH, on='lh_proc', how='inner')\n",
    "all_LH_inner.drop(columns=['lh_proc', 'last author', 'Authorship'], inplace=True)\n",
    "print(len(all_LH_inner))\n",
    "all_LH_inner.to_csv('preprocessed_data/LH_inner.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. First authors: TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_auth_pairs_FH = paper_auth_pairs[[\"first author\", \"Affiliation\", \"Country\", \"Ivy league\"]] # TODO\n",
    "paper_auth_pairs_FH = deduplicate_by(paper_auth_pairs_FH, \"first author\")\n",
    "claims_FH = authors_claims[authors_claims['Authorship'] == 'First']\n",
    "\n",
    "# create merge columns: lowercased and stripped of accents\n",
    "paper_auth_pairs_FH['fh_proc'] = paper_auth_pairs_FH['first author'].str.lower()\n",
    "paper_auth_pairs_FH['fh_proc'] = paper_auth_pairs_FH['fh_proc'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "claims_FH['fh_proc'] = claims_FH['Name'].str.lower()\n",
    "claims_FH['fh_proc'] = claims_FH['fh_proc'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "\n",
    "all_FH = pd.merge(claims_FH, paper_auth_pairs_FH, on='fh_proc', how='outer')\n",
    "print(len(claims_FH), len(paper_auth_pairs_FH), len(all_FH))\n",
    "\n",
    "unique_pairs = all_FH[[\"Name\", \"first author\", \"fh_proc\"]].drop_duplicates().sort_values(\"first author\", ascending=True)\n",
    "for i in range(0, len(unique_pairs)):\n",
    "    if pd.isna(unique_pairs.iloc[i]['first author']) or pd.isna(unique_pairs.iloc[i]['Name']):\n",
    "        print('ðŸ’¥ ', end='')\n",
    "        print(f\"{unique_pairs.iloc[i]['fh_proc']:<20} {unique_pairs.iloc[i]['first author']:<20}  {unique_pairs.iloc[i]['Name']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
