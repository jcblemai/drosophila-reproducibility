{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read an xlx file in pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Read Author info, which contains all the pairs\n",
    "paper_auth_pairs = pd.read_excel('input_data/2025-02-14_last_xlsx/1_Triage_Last author.xlsx', sheet_name='Tri sans les doublons')\n",
    "# Drop all columns with 'Unnamed' in the name\n",
    "paper_auth_pairs = paper_auth_pairs.drop(columns=[col for col in paper_auth_pairs.columns if 'Unnamed' in col]).drop(columns=['Source'])\n",
    "paper_auth_pairs.to_csv('input_data/2025-02-14_last_xlsx/1_Triage_Last author.csv', index=False)\n",
    "\n",
    "first_authors_claims = pd.read_excel('input_data/2025-02-14_last_xlsx/stats_author.xlsx', sheet_name='First')\n",
    "leading_authors_claims = pd.read_excel('input_data/2025-02-14_last_xlsx/stats_author.xlsx', sheet_name='Leading')\n",
    "leading_authors_claims[\"Authorship\"]= \"Leading\"\n",
    "first_authors_claims[\"Authorship\"]= \"First\"\n",
    "\n",
    "\n",
    "authors_claims = pd.concat([leading_authors_claims, first_authors_claims])\n",
    "authors_claims['Sex'] = authors_claims['Sex'].map({1: 'Male', 0: 'Female'})\n",
    "authors_claims = authors_claims.drop(columns=[col for col in authors_claims.columns if '%' in col])\n",
    "authors_claims.rename(columns={'Conituinity': 'Continuity'}, inplace=True)\n",
    "authors_claims['Historical lab'] = authors_claims['Historical lab'].astype('boolean')\n",
    "authors_claims['Continuity'] = authors_claims['Continuity'].astype('boolean')\n",
    "authors_claims[\"Partially Verified\"] = authors_claims[\"Partially verified\"]\n",
    "authors_claims = authors_claims.drop(columns=[\"Partially verified\"])\n",
    "authors_claims.to_csv('input_data/2025-02-14_last_xlsx/stats_author.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>last author</th>\n",
       "      <th>first author</th>\n",
       "      <th>Sex</th>\n",
       "      <th>PhD Post-doc</th>\n",
       "      <th>Become a Pi</th>\n",
       "      <th>current job</th>\n",
       "      <th>MD</th>\n",
       "      <th>Affiliation</th>\n",
       "      <th>Country</th>\n",
       "      <th>Ivy league</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agaisse H</td>\n",
       "      <td>DerrÃ©</td>\n",
       "      <td>0</td>\n",
       "      <td>Post-doc</td>\n",
       "      <td>1</td>\n",
       "      <td>PI</td>\n",
       "      <td>0</td>\n",
       "      <td>Yale University</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aguilera RJ</td>\n",
       "      <td>Seong CS</td>\n",
       "      <td>1</td>\n",
       "      <td>PhD</td>\n",
       "      <td>0</td>\n",
       "      <td>Academia</td>\n",
       "      <td>0</td>\n",
       "      <td>The University of Texas</td>\n",
       "      <td>USA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aigaki T</td>\n",
       "      <td>Tsuda M</td>\n",
       "      <td>1</td>\n",
       "      <td>PhD</td>\n",
       "      <td>1</td>\n",
       "      <td>Admin</td>\n",
       "      <td>0</td>\n",
       "      <td>Tokyo Metropolitan University</td>\n",
       "      <td>Japan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ando</td>\n",
       "      <td>Markus</td>\n",
       "      <td>1</td>\n",
       "      <td>PhD</td>\n",
       "      <td>0</td>\n",
       "      <td>Facility leader</td>\n",
       "      <td>0</td>\n",
       "      <td>Hungarian Academy of Sciences, Szeged</td>\n",
       "      <td>Hungary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ando</td>\n",
       "      <td>Rus F</td>\n",
       "      <td>0</td>\n",
       "      <td>PhD</td>\n",
       "      <td>0</td>\n",
       "      <td>??</td>\n",
       "      <td>0</td>\n",
       "      <td>Hungarian Academy of Sciences, Szeged</td>\n",
       "      <td>Hungary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>Yu XQ</td>\n",
       "      <td>Ao J</td>\n",
       "      <td>??</td>\n",
       "      <td>PhD</td>\n",
       "      <td>??</td>\n",
       "      <td>Senior Staff</td>\n",
       "      <td>0</td>\n",
       "      <td>niversity of Missouri-Kansas City</td>\n",
       "      <td>USA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>Zhu S</td>\n",
       "      <td>Yuan Y</td>\n",
       "      <td>1</td>\n",
       "      <td>PhD</td>\n",
       "      <td>0</td>\n",
       "      <td>Senior Staff</td>\n",
       "      <td>0</td>\n",
       "      <td>Institute of Zoology, Chinese Academy of Sciences</td>\n",
       "      <td>China</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>Zhu S</td>\n",
       "      <td>Tian C</td>\n",
       "      <td>0</td>\n",
       "      <td>PhD</td>\n",
       "      <td>1</td>\n",
       "      <td>PI</td>\n",
       "      <td>0</td>\n",
       "      <td>Institute of Zoology, Chinese Academy of Sciences</td>\n",
       "      <td>China</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>Zhu S</td>\n",
       "      <td>Zhang Z</td>\n",
       "      <td>0</td>\n",
       "      <td>PhD</td>\n",
       "      <td>0</td>\n",
       "      <td>Academia</td>\n",
       "      <td>0</td>\n",
       "      <td>Institute of Zoology, Chinese Academy of Sciences</td>\n",
       "      <td>China</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>Zhu S</td>\n",
       "      <td>Gao B</td>\n",
       "      <td>1</td>\n",
       "      <td>PhD</td>\n",
       "      <td>0</td>\n",
       "      <td>Academia</td>\n",
       "      <td>0</td>\n",
       "      <td>Institute of Zoology, Chinese Academy of Sciences</td>\n",
       "      <td>China</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     last author first author Sex PhD Post-doc Become a Pi      current job  \\\n",
       "0      Agaisse H        DerrÃ©   0     Post-doc           1               PI   \n",
       "1    Aguilera RJ     Seong CS   1          PhD           0         Academia   \n",
       "2       Aigaki T      Tsuda M   1          PhD           1            Admin   \n",
       "3           Ando       Markus   1          PhD           0  Facility leader   \n",
       "4           Ando        Rus F   0          PhD           0               ??   \n",
       "..           ...          ...  ..          ...         ...              ...   \n",
       "315        Yu XQ         Ao J  ??          PhD          ??     Senior Staff   \n",
       "316        Zhu S       Yuan Y   1          PhD           0     Senior Staff   \n",
       "317        Zhu S       Tian C   0          PhD           1               PI   \n",
       "318        Zhu S      Zhang Z   0          PhD           0         Academia   \n",
       "319        Zhu S        Gao B   1          PhD           0         Academia   \n",
       "\n",
       "    MD                                        Affiliation  Country  Ivy league  \n",
       "0    0                                    Yale University      USA           1  \n",
       "1    0                            The University of Texas      USA           0  \n",
       "2    0                      Tokyo Metropolitan University    Japan           0  \n",
       "3    0              Hungarian Academy of Sciences, Szeged  Hungary           0  \n",
       "4    0              Hungarian Academy of Sciences, Szeged  Hungary           0  \n",
       "..  ..                                                ...      ...         ...  \n",
       "315  0                  niversity of Missouri-Kansas City      USA           0  \n",
       "316  0  Institute of Zoology, Chinese Academy of Sciences    China           0  \n",
       "317  0  Institute of Zoology, Chinese Academy of Sciences    China           0  \n",
       "318  0  Institute of Zoology, Chinese Academy of Sciences    China           0  \n",
       "319  0  Institute of Zoology, Chinese Academy of Sciences    China           0  \n",
       "\n",
       "[320 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_auth_pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It seems that\n",
    "\n",
    " - sex -> FH\n",
    "\n",
    " - PhD Post-doc -> FH\n",
    "\n",
    " - Become a Pi -> FH\n",
    "\n",
    " - current job -> FH\n",
    "\n",
    " - MD -> **???**\n",
    "\n",
    " - Affiliation -> Both\n",
    "\n",
    " - Country -> Both\n",
    "\n",
    " - Ivy League -> Both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_by(df, col_name):\n",
    "    \"\"\"\n",
    "    Deduplicate a dataframe based on a specific column, keeping the most common values \n",
    "    for other columns when duplicates exist.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe to deduplicate\n",
    "    col_name : str\n",
    "        The column name to deduplicate by\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Deduplicated dataframe with one row per unique value in col_name\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    # Create a list to store unique values and their most common attribute values\n",
    "    unique_rows = []\n",
    "    \n",
    "    # Get unique values in the specified column\n",
    "    unique_values = df[col_name].unique()\n",
    "    \n",
    "    # For each unique value\n",
    "    for value in unique_values:\n",
    "        # Get all rows with this value\n",
    "        value_rows = df[df[col_name] == value]\n",
    "        \n",
    "        # Initialize a row for this unique value\n",
    "        unique_row = {col_name: value}\n",
    "        \n",
    "        # For each column except the one we're deduplicating by\n",
    "        for col in df.columns:\n",
    "            if col == col_name:\n",
    "                continue\n",
    "                \n",
    "            # Get the most common value\n",
    "            values = value_rows[col].dropna().tolist()\n",
    "            if len(values) == 0:\n",
    "                unique_row[col] = np.nan\n",
    "                continue\n",
    "                \n",
    "            # Use Counter to find the most common value\n",
    "            value_counts = Counter(values)\n",
    "            most_common_value, count = value_counts.most_common(1)[0]\n",
    "            \n",
    "            # Check if there are ties for most common value\n",
    "            if sum(1 for v, c in value_counts.items() if c == count) > 1:\n",
    "                print(f\"Warning: Multiple most common values for {value} in column {col}. Choosing {most_common_value}\")\n",
    "            \n",
    "            unique_row[col] = most_common_value\n",
    "        \n",
    "        unique_rows.append(unique_row)\n",
    "    \n",
    "    # Create a new DataFrame from the unique values\n",
    "    result_df = pd.DataFrame(unique_rows)\n",
    "    \n",
    "    # Reorder columns to match original DataFrame\n",
    "    result_df = result_df[df.columns]\n",
    "    \n",
    "    return result_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_auth_pairs_LH = paper_auth_pairs[[\"last author\", \"Affiliation\", \"Country\", \"Ivy league\"]]\n",
    "paper_auth_pairs_LH = deduplicate_by(paper_auth_pairs_LH, \"last author\")\n",
    "claims_LH = authors_claims[authors_claims['Authorship'] == 'Leading']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Multiple most common values for Kurucz E in column Affiliation. Choosing Hungarian Academy of Sciences, Szeged\n",
      "Warning: Multiple most common values for Kurucz E in column Country. Choosing Hungary\n",
      "Warning: Multiple most common values for Wang Z in column Affiliation. Choosing Brigham and Women's Hospital, Boston\n",
      "Warning: Multiple most common values for Wang Z in column Country. Choosing USA\n",
      "Warning: Multiple most common values for Basset A in column Affiliation. Choosing CNRS Gif-sur-Yvette UniversitÃ© Paris-Saclay\n",
      "Warning: Multiple most common values for De Gregorio E in column Affiliation. Choosing CNRS Gif-sur-Yvette UniversitÃ© Paris-Saclay\n",
      "Warning: Multiple most common values for Johansson KC in column Affiliation. Choosing Uppsala University\n",
      "Warning: Multiple most common values for Williams MJ in column Affiliation. Choosing University of Notre Dame\n",
      "Warning: Multiple most common values for Williams MJ in column Country. Choosing USA\n",
      "Warning: Multiple most common values for Franc NC in column Affiliation. Choosing Harvard Medical School,Boston\n",
      "Warning: Multiple most common values for RÃ¤met M in column Affiliation. Choosing Harvard Medical School,Boston\n",
      "Warning: Multiple most common values for Stuart LM in column Affiliation. Choosing Harvard Medical School,Boston\n",
      "Warning: Multiple most common values for Avet-Rochex A in column Affiliation. Choosing CEA Grenoble\n",
      "Warning: Multiple most common values for Gottar M in column Affiliation. Choosing CNRS-University of Strasbourg\n",
      "Warning: Multiple most common values for Gateff E in column Affiliation. Choosing Johannes Gutenberg UniversitÃ¤t\n",
      "Warning: Multiple most common values for Hedengren M in column Affiliation. Choosing University of Stockholm\n",
      "Warning: Multiple most common values for Hedengren M in column Country. Choosing Sweden\n",
      "Warning: Multiple most common values for Tzou P in column Affiliation. Choosing CNRS-University of Strasbourg\n",
      "Warning: Multiple most common values for Tanji T in column Affiliation. Choosing University of Massachusetts Medical School\n",
      "Warning: Multiple most common values for Tanji T in column Country. Choosing USA\n",
      "Warning: Multiple most common values for Goto A in column Affiliation. Choosing Tohoku University\n",
      "Warning: Multiple most common values for Goto A in column Country. Choosing Japan\n",
      "Warning: Multiple most common values for Kim YS in column Affiliation. Choosing Ewha Womans University\n",
      "Warning: Multiple most common values for Leulier F in column Affiliation. Choosing CNRS_Gif-sur-Yvette_UniversitÃ© Paris-Saclay\n",
      "Warning: Multiple most common values for Leulier F in column Country. Choosing France\n",
      "Warning: Multiple most common values for Scherfer C in column Affiliation. Choosing CNRS_Gif-sur-Yvette_UniversitÃ© Paris-Saclay\n",
      "Warning: Multiple most common values for Scherfer C in column Country. Choosing France\n",
      "Warning: Multiple most common values for Apidianakis Y in column Affiliation. Choosing Massachusetts General Hospital\n",
      "Warning: Multiple most common values for Zhang Z in column Affiliation. Choosing University of Kentucky\n",
      "Warning: Multiple most common values for Zhang Z in column Country. Choosing USA\n",
      "293 291 297\n",
      "ðŸ’¥ derre                DerrÃ©                 nan\n",
      "ðŸ’¥ markus               Markus                nan\n",
      "ðŸ’¥ rizki mt             RIZKI MT              nan\n",
      "ðŸ’¥ schneider d          Schneider D           nan\n",
      "ðŸ’¥ markus r             nan                   MÃ¡rkus R\n",
      "ðŸ’¥ hedengren-olcott m   nan                   Hedengren-Olcott M\n",
      "ðŸ’¥ derre i              nan                   DerrÃ© I\n",
      "ðŸ’¥ bellotti ra          nan                   Bellotti RA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pj/15hxgl0j1wg2w_t5k7bvnq5c0000gn/T/ipykernel_32517/3122261078.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  claims_FH['fh_proc'] = claims_FH['Name'].str.lower()\n",
      "/var/folders/pj/15hxgl0j1wg2w_t5k7bvnq5c0000gn/T/ipykernel_32517/3122261078.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  claims_FH['fh_proc'] = claims_FH['fh_proc'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n"
     ]
    }
   ],
   "source": [
    "paper_auth_pairs_FH = paper_auth_pairs[[\"first author\", \"Affiliation\", \"Country\", \"Ivy league\"]] # TODO\n",
    "paper_auth_pairs_FH = deduplicate_by(paper_auth_pairs_FH, \"first author\")\n",
    "claims_FH = authors_claims[authors_claims['Authorship'] == 'First']\n",
    "\n",
    "# create merge columns: lowercased and stripped of accents\n",
    "paper_auth_pairs_FH['fh_proc'] = paper_auth_pairs_FH['first author'].str.lower()\n",
    "paper_auth_pairs_FH['fh_proc'] = paper_auth_pairs_FH['fh_proc'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "claims_FH['fh_proc'] = claims_FH['Name'].str.lower()\n",
    "claims_FH['fh_proc'] = claims_FH['fh_proc'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "\n",
    "all_FH = pd.merge(claims_FH, paper_auth_pairs_FH, on='fh_proc', how='outer')\n",
    "print(len(claims_FH), len(paper_auth_pairs_FH), len(all_FH))\n",
    "\n",
    "unique_pairs = all_FH[[\"Name\", \"first author\", \"fh_proc\"]].drop_duplicates().sort_values(\"first author\", ascending=True)\n",
    "for i in range(0, len(unique_pairs)):\n",
    "    if pd.isna(unique_pairs.iloc[i]['first author']) or pd.isna(unique_pairs.iloc[i]['Name']):\n",
    "        print('ðŸ’¥ ', end='')\n",
    "        print(f\"{unique_pairs.iloc[i]['fh_proc']:<20} {unique_pairs.iloc[i]['first author']:<20}  {unique_pairs.iloc[i]['Name']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 161 163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pj/15hxgl0j1wg2w_t5k7bvnq5c0000gn/T/ipykernel_32517/2810914933.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  claims_LH['lh_proc'] = claims_LH['Name'].str.lower()\n",
      "/var/folders/pj/15hxgl0j1wg2w_t5k7bvnq5c0000gn/T/ipykernel_32517/2810914933.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  claims_LH['lh_proc'] = claims_LH['lh_proc'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
      "/var/folders/pj/15hxgl0j1wg2w_t5k7bvnq5c0000gn/T/ipykernel_32517/2810914933.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  claims_LH['lh_proc'] = claims_LH['lh_proc'].str.replace('ando i', 'ando')\n"
     ]
    }
   ],
   "source": [
    "# create merge columns: lowercased and stripped of accents\n",
    "paper_auth_pairs_LH['lh_proc'] = paper_auth_pairs_LH['last author'].str.lower()\n",
    "paper_auth_pairs_LH['lh_proc'] = paper_auth_pairs_LH['lh_proc'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "claims_LH['lh_proc'] = claims_LH['Name'].str.lower()\n",
    "claims_LH['lh_proc'] = claims_LH['lh_proc'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "# replace ando i by ando\n",
    "claims_LH['lh_proc'] = claims_LH['lh_proc'].str.replace('ando i', 'ando')\n",
    "\n",
    "all_LH = pd.merge(claims_LH, paper_auth_pairs_LH, on='lh_proc', how='outer')\n",
    "print(len(claims_LH), len(paper_auth_pairs_LH), len(all_LH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¥ bellotti ra          Bellotti RA           nan\n",
      "ðŸ’¥ boccard f            Boccard F             nan\n",
      "ðŸ’¥ brey pt              Brey PT               nan\n",
      "ðŸ’¥ cerenius l           Cerenius L            nan\n",
      "ðŸ’¥ higgins de           Higgins DE            nan\n",
      "ðŸ’¥ mengin-lecreulx d    Mengin-Lecreulx D     nan\n",
      "ðŸ’¥ moore kj             Moore KJ              nan\n",
      "ðŸ’¥ rizki mt             RIZKI MT              nan\n",
      "ðŸ’¥ shahabuddin m        Shahabuddin M         nan\n",
      "ðŸ’¥ shirasu-hiza mm      Shirasu-Hiza MM       nan\n",
      "ðŸ’¥ silvers mj           Silvers MJ            nan\n",
      "ðŸ’¥ matova n             nan                   Matova N\n",
      "ðŸ’¥ nappi aj             nan                   Nappi AJ\n"
     ]
    }
   ],
   "source": [
    "unique_pairs = all_LH[[\"Name\", \"last author\", \"lh_proc\"]].drop_duplicates().sort_values(\"last author\", ascending=True)\n",
    "for i in range(0, len(unique_pairs)):\n",
    "    if pd.isna(unique_pairs.iloc[i]['last author']) or pd.isna(unique_pairs.iloc[i]['Name']):\n",
    "        print('ðŸ’¥ ', end='')\n",
    "        print(f\"{unique_pairs.iloc[i]['lh_proc']:<20} {unique_pairs.iloc[i]['last author']:<20}  {unique_pairs.iloc[i]['Name']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "all_LH_inner = pd.merge(claims_LH, paper_auth_pairs_LH, on='lh_proc', how='inner')\n",
    "print(len(all_LH_inner))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Let's go for last authors first\n",
    "\n",
    " There are two ways to do that. By paper or by author. Perhaps by author ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Name   Sex  Historical lab  Continuity  Articles  Major claims  \\\n",
      "0    Lemaitre B  Male            True        True        24            69   \n",
      "1    Hultmark D  Male            True        True        28            67   \n",
      "2   Hoffmann JA  Male            True        True        16            45   \n",
      "3  Schneider DS  Male           False       False        12            31   \n",
      "4        Lee WJ  Male           False        True        12            31   \n",
      "\n",
      "   Unchallenged  Verified  Mixed  Challenged Authorship  Partially Verified  \\\n",
      "0             8        55      0           1    Leading                   5   \n",
      "1            12        47      0           2    Leading                   6   \n",
      "2             2        39      0           2    Leading                   2   \n",
      "3            17         6      0           7    Leading                   1   \n",
      "4             8        14      2           4    Leading                   3   \n",
      "\n",
      "        lh_proc   last author                                  Affiliation  \\\n",
      "0    lemaitre b    Lemaitre B  CNRS_Gif-sur-Yvette_UniversitÃ© Paris-Saclay   \n",
      "1    hultmark d    Hultmark D                      University of Stockholm   \n",
      "2   hoffmann ja   Hoffmann JA                CNRS-University of Strasbourg   \n",
      "3  schneider ds  Schneider DS                 Stanford University Stanford   \n",
      "4        lee wj        Lee WJ                       Ewha Womans University   \n",
      "\n",
      "  Country  Ivy league  \n",
      "0  France           0  \n",
      "1  Sweden           0  \n",
      "2  France           0  \n",
      "3     USA           1  \n",
      "4   Korea           0  \n"
     ]
    }
   ],
   "source": [
    "print(all_LH_inner.head())\n",
    "df = all_LH_inner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'analysis_claims'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KMeans\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatches\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpatches\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01manalysis_claims\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ASSESSMENT_COLORS\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Set global plotting parameters\u001b[39;00m\n\u001b[1;32m     17\u001b[0m plt\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39muse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseaborn-v0_8-whitegrid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'analysis_claims'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from analysis_claims import ASSESSMENT_COLORS\n",
    "\n",
    "\n",
    "# Set global plotting parameters\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assume df is already loaded\n",
    "# First, let's create proportions for each assessment type for each PI\n",
    "assessment_columns = ['Unchallenged', 'Verified', 'Partially Verified', 'Mixed', 'Challenged']\n",
    "\n",
    "for col in assessment_columns:\n",
    "    df[f'{col}_prop'] = df[col] / df['Major claims']\n",
    "\n",
    "# Let's also create a \"reproducibility score\" - higher means more verified claims\n",
    "df['reproducibility_score'] = (df['Verified_prop'] * 2 + df['Partially Verified_prop'] - \n",
    "                              df['Challenged_prop'] * 2 - df['Mixed_prop'] * 0.5)\n",
    "\n",
    "# Let's examine the distribution of this score\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['reproducibility_score'], kde=True)\n",
    "plt.title('Distribution of Reproducibility Scores')\n",
    "plt.xlabel('Reproducibility Score')\n",
    "plt.axvline(df['reproducibility_score'].mean(), color='red', linestyle='--', \n",
    "            label=f'Mean = {df[\"reproducibility_score\"].mean():.2f}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Basic descriptive statistics\n",
    "descriptive_stats = df[assessment_columns + [col + '_prop' for col in assessment_columns] + \n",
    "                     ['reproducibility_score', 'Articles', 'Major claims']].describe()\n",
    "print(\"Descriptive Statistics:\")\n",
    "descriptive_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSESSMENT_COLORS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSESSMENT_COLORS[category]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by historical lab status\n",
    "historical_grouped = df.groupby('Historical lab').agg({\n",
    "    **{col: 'sum' for col in assessment_columns},\n",
    "    'Major claims': 'sum',\n",
    "    'Articles': 'sum',\n",
    "    'reproducibility_score': 'mean'\n",
    "})\n",
    "\n",
    "# Calculate proportions\n",
    "for col in assessment_columns:\n",
    "    historical_grouped[f'{col}_prop'] = historical_grouped[col] / historical_grouped['Major claims']\n",
    "\n",
    "# Create stacked bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Prepare data for stacking\n",
    "data = []\n",
    "categories = []\n",
    "for col in ['Verified_prop', 'Partially Verified_prop', 'Unchallenged_prop', 'Mixed_prop', 'Challenged_prop']:\n",
    "    data.append(historical_grouped[col].values)\n",
    "    categories.append(col.replace('_prop', ''))\n",
    "\n",
    "# Create stacked bars\n",
    "x = np.arange(len(historical_grouped.index))\n",
    "bottom = np.zeros(len(historical_grouped.index))\n",
    "bars = []\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    bar = ax.bar(x, data[i], bottom=bottom, label=category, \n",
    "                color=ASSESSMENT_COLORS[category])\n",
    "    bottom += data[i]\n",
    "    bars.append(bar)\n",
    "\n",
    "# Add data labels to each segment\n",
    "for bar in bars:\n",
    "    for rect in bar:\n",
    "        height = rect.get_height()\n",
    "        if height > 0.05:  # Only add label if segment is large enough\n",
    "            ax.text(rect.get_x() + rect.get_width()/2., rect.get_y() + height/2.,\n",
    "                  f'{height:.2f}', ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "# Add number of papers as text on bars\n",
    "for i, lab in enumerate(historical_grouped.index):\n",
    "    ax.text(i, 1.02, f\"n={historical_grouped.loc[lab, 'Articles']}\", ha='center')\n",
    "\n",
    "# Customize plot\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Non-traditional Labs', 'Traditional-Historical Labs'])\n",
    "ax.set_ylabel('Proportion of Claims')\n",
    "ax.set_title('Assessment of Scientific Claims by Lab Tradition')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "# Add statistical annotation\n",
    "t_stat, p_val = stats.ttest_ind(\n",
    "    df[df['Historical lab'] == True]['reproducibility_score'].dropna(),\n",
    "    df[df['Historical lab'] == False]['reproducibility_score'].dropna(),\n",
    "    equal_var=False\n",
    ")\n",
    "\n",
    "sign = \"*\" if p_val < 0.05 else \"ns\"\n",
    "ax.text(0.5, 1.08, f\"Reproducibility score difference: t={t_stat:.2f}, p={p_val:.3f} {sign}\", \n",
    "      ha='center', transform=ax.transAxes, fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"Summary of Traditional vs. Non-traditional Labs:\")\n",
    "print(historical_grouped[['Major claims', 'Articles', 'reproducibility_score', \n",
    "                        'Verified_prop', 'Challenged_prop', 'Unchallenged_prop']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by country and calculate proportions\n",
    "country_grouped = df.groupby('Country').agg({\n",
    "    **{col: 'sum' for col in assessment_columns},\n",
    "    'Major claims': 'sum',\n",
    "    'Articles': 'count',\n",
    "    'reproducibility_score': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate proportions\n",
    "for col in assessment_columns:\n",
    "    country_grouped[f'{col}_prop'] = country_grouped[col] / country_grouped['Major claims']\n",
    "\n",
    "# Filter to include only countries with sufficient data (at least 5 PIs)\n",
    "country_filtered = country_grouped[country_grouped['Articles'] >= 5]\n",
    "\n",
    "# Sort by reproducibility score\n",
    "country_filtered = country_filtered.sort_values('reproducibility_score', ascending=False)\n",
    "\n",
    "# Create a horizontal grouped bar chart\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "# Set width and positions\n",
    "width = 0.15\n",
    "x = np.arange(len(country_filtered))\n",
    "\n",
    "# Plot each assessment type\n",
    "for i, category in enumerate(['Verified_prop', 'Partially Verified_prop', 'Unchallenged_prop', \n",
    "                             'Mixed_prop', 'Challenged_prop']):\n",
    "    ax.barh(x + i*width - 0.3, country_filtered[category], width, \n",
    "           label=category.replace('_prop', ''),\n",
    "           color=ASSESSMENT_COLORS[category.replace('_prop', '')])\n",
    "\n",
    "# Customize plot\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(country_filtered['Country'])\n",
    "ax.set_xlabel('Proportion of Claims')\n",
    "ax.set_title('Scientific Claim Assessment by Country')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "# Add sample size annotation\n",
    "for i, country in enumerate(country_filtered['Country']):\n",
    "    ax.text(0, i - 0.4, f\"n={country_filtered.iloc[i]['Articles']} PIs, {country_filtered.iloc[i]['Major claims']} claims\", \n",
    "           fontsize=10)\n",
    "\n",
    "# Add ANOVA results for reproducibility score differences by country\n",
    "f_stat, p_val = stats.f_oneway(\n",
    "    *[df[df['Country'] == country]['reproducibility_score'].dropna() \n",
    "     for country in country_filtered['Country']]\n",
    ")\n",
    "\n",
    "ax.text(0.5, -0.08, f\"ANOVA for reproducibility score by country: F={f_stat:.2f}, p={p_val:.3f}\", \n",
    "      ha='center', transform=ax.transAxes, fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print numerical summary\n",
    "print(\"Country Rankings by Reproducibility Score:\")\n",
    "print(country_filtered[['Country', 'Articles', 'Major claims', 'reproducibility_score',\n",
    "                       'Verified_prop', 'Challenged_prop', 'Unchallenged_prop']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by continuity in the field\n",
    "continuity_grouped = df.groupby('Continuity').agg({\n",
    "    **{col: 'sum' for col in assessment_columns},\n",
    "    'Major claims': 'sum',\n",
    "    'Articles': 'sum',\n",
    "    'reproducibility_score': 'mean'\n",
    "})\n",
    "\n",
    "# Calculate proportions\n",
    "for col in assessment_columns:\n",
    "    continuity_grouped[f'{col}_prop'] = continuity_grouped[col] / continuity_grouped['Major claims']\n",
    "\n",
    "# Create a radar chart to compare the profiles\n",
    "categories = ['Verified_prop', 'Partially Verified_prop', 'Unchallenged_prop', \n",
    "             'Mixed_prop', 'Challenged_prop']\n",
    "categories_clean = [cat.replace('_prop', '') for cat in categories]\n",
    "\n",
    "# Number of variables\n",
    "N = len(categories)\n",
    "\n",
    "# What will be the angle of each axis\n",
    "angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "angles += angles[:1]  # Close the loop\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "\n",
    "# Draw one axis per variable + add labels\n",
    "plt.xticks(angles[:-1], categories_clean, size=14)\n",
    "\n",
    "# Draw the data for established researchers\n",
    "values_true = continuity_grouped.loc[True, categories].values.flatten().tolist()\n",
    "values_true += values_true[:1]  # Close the loop\n",
    "ax.plot(angles, values_true, linewidth=3, linestyle='solid', \n",
    "       label='Established Researchers', color='#2ecc71')\n",
    "ax.fill(angles, values_true, alpha=0.25, color='#2ecc71')\n",
    "\n",
    "# Draw the data for newcomers\n",
    "values_false = continuity_grouped.loc[False, categories].values.flatten().tolist()\n",
    "values_false += values_false[:1]  # Close the loop\n",
    "ax.plot(angles, values_false, linewidth=3, linestyle='dashed', \n",
    "       label='Newcomers', color='#e74c3c')\n",
    "ax.fill(angles, values_false, alpha=0.25, color='#e74c3c')\n",
    "\n",
    "# Add legend and title\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "plt.title('Claim Assessment Profile: Newcomers vs. Established Researchers', size=18, y=1.1)\n",
    "\n",
    "# Customize radar chart\n",
    "ax.set_ylim(0, max(max(values_true), max(values_false)) * 1.1)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add sample size annotation\n",
    "plt.annotate(f\"Established: n={continuity_grouped.loc[True, 'Articles']} articles, {continuity_grouped.loc[True, 'Major claims']} claims\", \n",
    "            xy=(0.5, -0.1), xycoords='axes fraction', ha='center')\n",
    "plt.annotate(f\"Newcomers: n={continuity_grouped.loc[False, 'Articles']} articles, {continuity_grouped.loc[False, 'Major claims']} claims\", \n",
    "            xy=(0.5, -0.15), xycoords='axes fraction', ha='center')\n",
    "\n",
    "# Add t-test results\n",
    "t_stat, p_val = stats.ttest_ind(\n",
    "    df[df['Continuity'] == True]['reproducibility_score'].dropna(),\n",
    "    df[df['Continuity'] == False]['reproducibility_score'].dropna()\n",
    ")\n",
    "plt.annotate(f\"Reproducibility score t-test: t={t_stat:.2f}, p={p_val:.3f}\", \n",
    "            xy=(0.5, -0.2), xycoords='axes fraction', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a second visualization showing differences in unchallenged claims\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Bar chart for unchallenged claims\n",
    "unchallenged_data = [continuity_grouped.loc[False, 'Unchallenged_prop'], \n",
    "                    continuity_grouped.loc[True, 'Unchallenged_prop']]\n",
    "challenged_data = [continuity_grouped.loc[False, 'Challenged_prop'], \n",
    "                  continuity_grouped.loc[True, 'Challenged_prop']]\n",
    "\n",
    "x = np.arange(2)\n",
    "width = 0.35\n",
    "\n",
    "unchallenged_bars = ax.bar(x - width/2, unchallenged_data, width, \n",
    "                         label='Unchallenged', color=ASSESSMENT_COLORS['Unchallenged'])\n",
    "challenged_bars = ax.bar(x + width/2, challenged_data, width, \n",
    "                       label='Challenged', color=ASSESSMENT_COLORS['Challenged'])\n",
    "\n",
    "# Add data labels\n",
    "for bars in [unchallenged_bars, challenged_bars]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "               f'{height:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# Customize plot\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Newcomers', 'Established Researchers'])\n",
    "ax.set_ylabel('Proportion of Claims')\n",
    "ax.set_title('Unchallenged vs. Challenged Claims by Field Experience')\n",
    "ax.legend()\n",
    "\n",
    "# Add statistical annotation\n",
    "t_stat_unch, p_val_unch = stats.ttest_ind(\n",
    "    df[df['Continuity'] == False]['Unchallenged_prop'].dropna(),\n",
    "    df[df['Continuity'] == True]['Unchallenged_prop'].dropna()\n",
    ")\n",
    "ax.text(0.5, 0.95, f\"Unchallenged proportion t-test: t={t_stat_unch:.2f}, p={p_val_unch:.3f}\", \n",
    "       ha='center', transform=ax.transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"Summary of Newcomers vs. Established Researchers:\")\n",
    "continuity_summary = continuity_grouped.copy()\n",
    "continuity_summary.index = ['Newcomers', 'Established Researchers']\n",
    "print(continuity_summary[['Major claims', 'Articles', 'reproducibility_score', \n",
    "                         'Verified_prop', 'Challenged_prop', 'Unchallenged_prop']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multivariable regression model to predict reproducibility\n",
    "# First, ensure no object datatypes are in our dataset by examining the data\n",
    "print(\"Data types before preprocessing:\")\n",
    "print(df[['Historical lab', 'Continuity', 'Ivy league', 'Articles', \n",
    "          'Sex', 'Unchallenged_prop', 'Challenged_prop', 'reproducibility_score']].dtypes)\n",
    "\n",
    "# Convert any object columns to appropriate types\n",
    "# Make sure all numeric columns are properly formatted\n",
    "numeric_cols = ['Articles', 'Unchallenged_prop', 'Challenged_prop', 'reproducibility_score']\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Convert boolean columns to integer\n",
    "bool_cols = ['Historical lab', 'Continuity']\n",
    "for col in bool_cols:\n",
    "    df[col] = df[col].astype(int)\n",
    "    \n",
    "# Make sure categorical variables are properly typed\n",
    "if 'Sex' in df.columns:\n",
    "    df['Sex'] = df['Sex'].astype('category')\n",
    "    \n",
    "# Create dummy variables properly\n",
    "df_model = pd.get_dummies(df[['Historical lab', 'Continuity', 'Ivy league', \n",
    "                             'Articles', 'Unchallenged_prop', 'Challenged_prop', \n",
    "                             'reproducibility_score']])\n",
    "\n",
    "# Print data types after preprocessing\n",
    "print(\"Data types after preprocessing:\")\n",
    "print(df_model.dtypes)\n",
    "\n",
    "# Fit regression model for unchallenged proportion\n",
    "# Select predictors (X) and target (y)\n",
    "y = df_model['Unchallenged_prop']\n",
    "X = df_model.drop(['Unchallenged_prop', 'Challenged_prop', 'reproducibility_score'], axis=1)\n",
    "\n",
    "# Add constant\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Verify no object datatypes remain\n",
    "print(\"X dtypes:\", X.dtypes.unique())\n",
    "print(\"y dtype:\", y.dtype)\n",
    "\n",
    "# Fit model\n",
    "try:\n",
    "    model_unchallenged = sm.OLS(y, X).fit()\n",
    "    print(\"Model successfully fit!\")\n",
    "    \n",
    "    # Create another model for reproducibility score\n",
    "    y2 = df_model['reproducibility_score']\n",
    "    model_repro = sm.OLS(y2, X).fit()\n",
    "    \n",
    "    # Print summaries\n",
    "    print(\"\\nRegression Model for Unchallenged Proportion:\")\n",
    "    print(model_unchallenged.summary())\n",
    "    \n",
    "    print(\"\\nRegression Model for Reproducibility Score:\")\n",
    "    print(model_repro.summary())\n",
    "    \n",
    "    # Create a visual summary of the regression results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "    \n",
    "    # Extract coefficients and confidence intervals for unchallenged model\n",
    "    coefs1 = model_unchallenged.params[1:]\n",
    "    conf_intervals1 = model_unchallenged.conf_int().iloc[1:]\n",
    "    errors1 = (conf_intervals1[1] - conf_intervals1[0]) / 3.92  # 95% CI to standard error\n",
    "    \n",
    "    # Plot coefficients for unchallenged model\n",
    "    ax1.errorbar(coefs1, range(len(coefs1)), xerr=errors1, fmt='o', capsize=5, color='#3498db')\n",
    "    ax1.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax1.set_yticks(range(len(coefs1)))\n",
    "    ax1.set_yticklabels(coefs1.index)\n",
    "    ax1.set_xlabel('Coefficient Value')\n",
    "    ax1.set_title('Predictors of Unchallenged Claims')\n",
    "    \n",
    "    # Extract coefficients and confidence intervals for reproducibility model\n",
    "    coefs2 = model_repro.params[1:]\n",
    "    conf_intervals2 = model_repro.conf_int().iloc[1:]\n",
    "    errors2 = (conf_intervals2[1] - conf_intervals2[0]) / 3.92  # 95% CI to standard error\n",
    "    \n",
    "    # Plot coefficients for reproducibility model\n",
    "    ax2.errorbar(coefs2, range(len(coefs2)), xerr=errors2, fmt='o', capsize=5, color='#2ecc71')\n",
    "    ax2.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax2.set_yticks(range(len(coefs2)))\n",
    "    ax2.set_yticklabels(coefs2.index)\n",
    "    ax2.set_xlabel('Coefficient Value')\n",
    "    ax2.set_title('Predictors of Reproducibility Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error fitting model: {e}\")\n",
    "    print(\"Let's try an alternative approach using a simpler model\")\n",
    "    \n",
    "    # Create a simpler model with fewer variables\n",
    "    basic_vars = ['Articles', 'Historical lab', 'Continuity']\n",
    "    df_simple = df[basic_vars + ['Unchallenged_prop']].copy()\n",
    "    \n",
    "    for col in basic_vars + ['Unchallenged_prop']:\n",
    "        df_simple[col] = pd.to_numeric(df_simple[col], errors='coerce')\n",
    "    \n",
    "    # Drop missing values\n",
    "    df_simple = df_simple.dropna()\n",
    "    \n",
    "    # Run simpler regression\n",
    "    X_simple = sm.add_constant(df_simple[basic_vars])\n",
    "    y_simple = df_simple['Unchallenged_prop']\n",
    "    \n",
    "    model_simple = sm.OLS(y_simple, X_simple).fit()\n",
    "    print(\"\\nSimplified Regression Model:\")\n",
    "    print(model_simple.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_csv('test.csv')\n",
    "\n",
    "# Calculate percentage of challenged claims for each author\n",
    "df['challenged_rate'] = df['Challenged'] / df['Major claims'] * 100\n",
    "\n",
    "# Function for statistical testing\n",
    "def compare_groups(data, column, rate_column='challenged_rate'):\n",
    "    groups = data[column].unique()\n",
    "    if len(groups) == 2:  # For binary variables like Sex\n",
    "        group1 = data[data[column] == groups[0]][rate_column]\n",
    "        group2 = data[data[column] == groups[1]][rate_column]\n",
    "        stat, pval = stats.mannwhitneyu(group1, group2)\n",
    "        return {\n",
    "            'groups': groups,\n",
    "            'medians': [group1.median(), group2.median()],\n",
    "            'p_value': pval\n",
    "        }\n",
    "    else:  # For variables with more than 2 categories\n",
    "        stat, pval = stats.kruskal(*[group[rate_column].values \n",
    "                                    for name, group in data.groupby(column)])\n",
    "        return {\n",
    "            'groups': groups,\n",
    "            'medians': [group[rate_column].median() \n",
    "                       for name, group in data.groupby(column)],\n",
    "            'p_value': pval\n",
    "        }\n",
    "\n",
    "# Create subplots for our analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "\n",
    "# 1. Sex Analysis\n",
    "sns.boxplot(data=df, x='Sex', y='challenged_rate', ax=axes[0,0])\n",
    "sex_stats = compare_groups(df, 'Sex')\n",
    "axes[0,0].set_title(f'Challenged Claims Rate by Sex\\np={sex_stats[\"p_value\"]:.3f}')\n",
    "\n",
    "# 2. Historical Lab Analysis\n",
    "sns.boxplot(data=df, x='Historical lab', y='challenged_rate', ax=axes[0,1])\n",
    "lab_stats = compare_groups(df, 'Historical lab')\n",
    "axes[0,1].set_title(f'Challenged Claims Rate by Historical Lab\\np={lab_stats[\"p_value\"]:.3f}')\n",
    "\n",
    "# 3. Country Analysis\n",
    "sns.boxplot(data=df, x='Country', y='challenged_rate', ax=axes[1,0])\n",
    "country_stats = compare_groups(df, 'Country')\n",
    "axes[1,0].set_title(f'Challenged Claims Rate by Country\\np={country_stats[\"p_value\"]:.3f}')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Ivy League vs Non-Ivy League\n",
    "sns.boxplot(data=df, x='Ivy league', y='challenged_rate', ax=axes[1,1])\n",
    "ivy_stats = compare_groups(df, 'Ivy league')\n",
    "axes[1,1].set_title(f'Challenged Claims Rate by Ivy League Status\\np={ivy_stats[\"p_value\"]:.3f}')\n",
    "\n",
    "# Add overall title\n",
    "plt.suptitle('Factors Affecting Rate of Challenged Claims', fontsize=16, y=1.02)\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "for factor in ['Sex', 'Historical lab', 'Country', 'Ivy league']:\n",
    "    stats_result = compare_groups(df, factor)\n",
    "    print(f\"\\n{factor}:\")\n",
    "    for group, median in zip(stats_result['groups'], stats_result['medians']):\n",
    "        print(f\"{group}: Median challenged rate = {median:.2f}%\")\n",
    "    print(f\"p-value = {stats_result['p_value']:.3f}\")\n",
    "\n",
    "# Additional analysis for continuous relationships\n",
    "if 'Continuity' in df.columns:\n",
    "    correlation = stats.spearmanr(df['Continuity'], df['challenged_rate'])\n",
    "    print(\"\\nContinuity correlation:\")\n",
    "    print(f\"Spearman correlation = {correlation.correlation:.3f}\")\n",
    "    print(f\"p-value = {correlation.pvalue:.3f}\")\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('challenged_claims_analysis.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "# Create a summary table\n",
    "summary_df = df.groupby(['Sex', 'Historical lab', 'Country']).agg({\n",
    "    'challenged_rate': ['mean', 'median', 'std', 'count']\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nDetailed Summary Table:\")\n",
    "print(summary_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
